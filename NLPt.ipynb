{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import  nn,optim\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"./models/google-bert/bert-base-uncased\"\n",
    "from transformers import get_scheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3127b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "df=pd.read_csv(\"cybersecurity_attacks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看数据相关信息\n",
    "print(df.columns)\n",
    "\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53641a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 空数据处理\n",
    "df['Alerts/Warnings']=df['Alerts/Warnings'].apply(lambda x:'None' if pd.isna(x) else x)\n",
    "df['IDS/IPS Alerts'] = df['IDS/IPS Alerts'].apply(lambda x: 'No Data' if pd.isna(x) else x)\n",
    "df['Malware Indicators'] = df['Malware Indicators'].apply(lambda x: 'No Detection' if pd.isna(x) else x)\n",
    "df['Firewall Logs'] = df['Firewall Logs'].apply(lambda x: 'No Data'if pd.isna(x) else x)\n",
    "df['Proxy Information'] = df['Proxy Information'].apply(lambda x: 'No Proxy Data' if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e0f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "devices = [\n",
    "    r'Windows',\n",
    "    r'Linux',\n",
    "    r'Android',\n",
    "    r'iPad',\n",
    "    r'iPod',\n",
    "    r'iPhone',\n",
    "    r'Macintosh']\n",
    "\n",
    "\n",
    "def device_os_finder(user_agent):\n",
    "    for device in devices:\n",
    "        match_device = re.search(device, user_agent, re.I)  # re.I makes the search case-insensitive\n",
    "        if match_device:\n",
    "            return match_device.group()\n",
    "    return 'Unknown'\n",
    "\n",
    "# Extract device or OS\n",
    "df['Device'] = df['Device Information'].apply(device_os_finder)\n",
    "df['Browser'] = df['Device Information'].str.split('/').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Source IP Address']\n",
    "del df['Destination IP Address']\n",
    "del df['Timestamp']\n",
    "del df['Log Source']\n",
    "del df['User Information']\n",
    "del df['Device Information']\n",
    "del df['Geo-location Data']\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2994f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# 获取设备信息的统计数量\n",
    "device_counts = df['Device'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=device_counts.index, y=device_counts.values, palette='Set2')\n",
    "\n",
    "plt.title('Device  Distribution')\n",
    "plt.xlabel('Device Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123648d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count occurrences of each combination of PROTOCOL and ATTACK TYPE\n",
    "protocol_attack_counts = df.groupby(['Protocol', 'Attack Type']).size().reset_index(name='COUNT')\n",
    "\n",
    "# Sort by count in descending order and take top 10 for better visibility\n",
    "top_10 = protocol_attack_counts.sort_values('COUNT', ascending=False).head(10)\n",
    "\n",
    "# Print a summary of the data\n",
    "print(top_10)\n",
    "\n",
    "# Create a pie chart\n",
    "#ax = attack_counts.plot(kind='bar', stacked=True, figsize=(12, 6))\n",
    "plt.plot(figsize=(12, 8))\n",
    "plt.pie(top_10['COUNT'], labels=top_10.apply(lambda x: f\"{x['Protocol']} - {x['Attack Type']}\", axis=1), \n",
    "        autopct='%1.1f%%', startangle=90, pctdistance=0.85)\n",
    "\n",
    "# Add a circle at the center to create a donut chart (optional)\n",
    "center_circle = plt.Circle((0,0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(center_circle)\n",
    "\n",
    "# Add title\n",
    "plt.title('Top 10 Protocol-Attack Type Combinations')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title='Protocol - Attack Type', loc='center left', bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "# Adjust layout to prevent cutting off labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5eb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore relationships between features and the target variable\n",
    "# Select some features for visualization. Adjust based on actual features in your dataset.\n",
    "features = ['Source Port', 'Destination Port', 'Packet Length', 'Anomaly Scores']\n",
    "\n",
    "for feature in features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Attack Type', y=feature, data=df)\n",
    "    plt.title(f'{feature} vs. Attack Type')\n",
    "    plt.xlabel('Attack Type')\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b3451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建映射字典\n",
    "label_mapping = {'Malware': 0, 'DDoS': 1, 'Intrusion': 2}\n",
    "\n",
    "# 构建输入文本（包含所有列的数据）\n",
    "texts = [\n",
    "    f\"Source Port: {row['Source Port']}. \"\n",
    "    f\"Destination Port: {row['Destination Port']}. \"\n",
    "    f\"Protocol: {row['Protocol']}. \"\n",
    "    f\"Packet Length: {row['Packet Length']}. \"\n",
    "    f\"Packet Type: {row['Packet Type']}. \"\n",
    "    f\"Traffic Type: {row['Traffic Type']}. \"\n",
    "    f\"Payload Data: {row['Payload Data']}. \"\n",
    "    f\"Malware Indicators: {row['Malware Indicators']}. \"\n",
    "    f\"Anomaly Scores: {row['Anomaly Scores']}. \"\n",
    "    f\"Alerts/Warnings: {row['Alerts/Warnings']}. \"\n",
    "    f\"Attack Signature: {row['Attack Signature']}. \"\n",
    "    f\"Action Taken: {row['Action Taken']}. \"\n",
    "    f\"Severity Level: {row['Severity Level']}. \"\n",
    "    f\"Network Segment: {row['Network Segment']}. \"\n",
    "    f\"Proxy Information: {row['Proxy Information']}. \"\n",
    "    f\"Firewall Logs: {row['Firewall Logs']}. \"\n",
    "    f\"IDS/IPS Alerts: {row['IDS/IPS Alerts']}. \"\n",
    "    f\"Device: {row['Device']}. \"\n",
    "    f\"Browser: {row['Browser']}.\"\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# 标签\n",
    "label = [label_mapping[row['Attack Type']] for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记载bert模型和分词器\n",
    "bert_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ce9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看最长的token有多大\n",
    "max_len = 0\n",
    "for text in texts:\n",
    "    tokens = bert_tokenizer.tokenize(text)\n",
    "    if len(tokens) > max_len:\n",
    "        max_len = len(tokens)\n",
    "print(\"最长的token长度为：\", max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先划分训练集 (70%) 和临时集 (30%)\n",
    "texts_train, texts_temp, labels_train, labels_temp = train_test_split(\n",
    "    texts, label, test_size=0.3, random_state=42, stratify=label\n",
    ")\n",
    "\n",
    "# 再把临时集划分为验证集 (10%) 和测试集 (20%)，比例是 1:2\n",
    "texts_val, texts_test, labels_val, labels_test = train_test_split(\n",
    "    texts_temp, labels_temp, test_size=2 / 3, random_state=42, stratify=labels_temp\n",
    ")\n",
    "\n",
    "# 检查划分比例\n",
    "print(\"训练集样本数:\", len(texts_train) / 40000)\n",
    "print(\"验证集样本数:\", len(texts_val) / 40000)\n",
    "print(\"测试集样本数:\", len(texts_test) / 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bd1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "max_length = 200  # 可以根据文本长度调整\n",
    "\n",
    "\n",
    "# 自定义 Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # tokenizer 编码\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # 返回 dict，方便 DataLoader collate\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # [seq_len]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # [seq_len]\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# 选择合适 batch_size（典型 BERT 是 16/32，可根据 GPU 内存调整）\n",
    "batch_size = 32\n",
    "\n",
    "# 构建 Dataset\n",
    "train_dataset = TextDataset(texts_train, labels_train, bert_tokenizer, max_length)\n",
    "val_dataset = TextDataset(texts_val, labels_val, bert_tokenizer, max_length)\n",
    "test_dataset = TextDataset(texts_test, labels_test, bert_tokenizer, max_length)\n",
    "\n",
    "# 构建 DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "k = 0\n",
    "# 检查第一个 batch\n",
    "for batch in train_loader:\n",
    "    print(batch['input_ids'].shape)  # [batch_size, max_length]\n",
    "    print(batch['attention_mask'].shape)  # [batch_size, max_length]\n",
    "    print(batch['labels'].shape)  # [batch_size]\n",
    "    break\n",
    "\n",
    "print('train batch',len(train_loader))\n",
    "print('val batch',len(val_loader))\n",
    "print('test batch',len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce845b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels=3, dropout=0.3, unfreeze_last_n=0):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        hidden_size = (\n",
    "            self.bert.config.hidden_size if hasattr(self.bert, \"config\") else 768\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.act = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, num_labels)\n",
    "\n",
    "        # 默认先冻结所有 bert 参数，再按需解冻最后 n 层\n",
    "        for p in self.bert.parameters():\n",
    "            p.requires_grad = False\n",
    "        if unfreeze_last_n > 0:\n",
    "\n",
    "            # 适配 BertModel 的 encoder 结构\n",
    "            for layer in self.bert.encoder.layer[-unfreeze_last_n:]:\n",
    "                for p in layer.parameters():\n",
    "                    p.requires_grad = True\n",
    "            # 解冻 pooler\n",
    "            if hasattr(self.bert, \"pooler\"):\n",
    "                for p in self.bert.pooler.parameters():\n",
    "                    p.requires_grad = True\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        cls_emb = outputs.pooler_output\n",
    "\n",
    "        x = self.dropout(cls_emb)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型：示例解冻最后 2 层 bert（根据显存/数据量调整）\n",
    "unfreeze_last_n = 2\n",
    "my_model = MyModel(bert_model, num_labels=3, dropout=0.3, unfreeze_last_n=unfreeze_last_n).to(device)\n",
    "\n",
    "# 不同参数组：给 bert 和 head 不同学习率\n",
    "bert_lr = 2e-5\n",
    "head_lr = 2e-4\n",
    "bert_params = [p for n, p in my_model.named_parameters() if \"bert\" in n and p.requires_grad]\n",
    "head_params = [p for n, p in my_model.named_parameters() if \"bert\" not in n and p.requires_grad]\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": bert_params, \"lr\": bert_lr},\n",
    "    {\"params\": head_params, \"lr\": head_lr}\n",
    "])\n",
    "\n",
    "loss_fuc = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 3\n",
    "# 修正 num_training_steps：epochs * batches_per_epoch\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a64675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    my_model.train()\n",
    "    best_val_acc = 0.0\n",
    "    save_path = \"best_model.pt\"\n",
    "    from tqdm import tqdm\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_total = 0.0\n",
    "        train_samples = 0\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", unit=\"batch\") as pbar:\n",
    "            for batch in pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = my_model(input_ids, attention_mask)\n",
    "                loss = loss_fuc(out, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                train_loss_total += loss.item() * labels.size(0)\n",
    "                train_samples += labels.size(0)\n",
    "                out_label = out.argmax(dim=1)\n",
    "                accuracy = (out_label == labels).sum().item() / labels.size(0)\n",
    "                lr = optimizer.param_groups[0]['lr']\n",
    "                pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{accuracy:.3f}\", lr=f\"{lr:.6f}\")\n",
    "        avg_train_loss = train_loss_total / train_samples\n",
    "        # ------------------- 验证 -------------------\n",
    "        my_model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", unit=\"batch\"):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                out = my_model(input_ids, attention_mask)\n",
    "                loss = loss_fuc(out, labels)\n",
    "                val_loss_total += loss.item() * labels.size(0)\n",
    "                out_label = out.argmax(dim=1)\n",
    "                val_correct += (out_label == labels).sum().item()\n",
    "                val_samples += labels.size(0)\n",
    "        val_loss = val_loss_total / val_samples\n",
    "        val_accuracy = val_correct / val_samples\n",
    "        print(f\"Epoch {epoch+1}: TrainLoss={avg_train_loss:.4f} ValLoss={val_loss:.4f} ValAcc={val_accuracy:.4f}\")\n",
    "        # 根据验证集保存最优模型（用于后续测试或调参）\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(my_model.state_dict(), save_path)\n",
    "            print(f\"Saved best model with val_acc={best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5591e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "my_model.eval()\n",
    "test_correct = 0\n",
    "test_samples = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\", unit=\"batch\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        out = my_model(input_ids, attention_mask)\n",
    "        out_label = out.argmax(dim=1)\n",
    "        test_correct += (out_label == labels).sum().item()\n",
    "        test_samples += labels.size(0)\n",
    "\n",
    "test_accuracy = test_correct / test_samples\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
